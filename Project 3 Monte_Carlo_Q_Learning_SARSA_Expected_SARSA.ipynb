{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "18/05/2023"
      ],
      "metadata": {
        "id": "KGWcPQTjJB5R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q-learning"
      ],
      "metadata": {
        "id": "9a38LdtkJH6m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XFJkt5E4I2WM"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "#Q-learning\n",
        "\n",
        "def smooth(y, box_pts):\n",
        "    box = np.ones(box_pts) / box_pts\n",
        "    y_smooth = np.convolve(y, box, mode='valid')\n",
        "    return y_smooth\n",
        "\n",
        "def plot_policy(q_table):\n",
        "    actions = ['←', '↓', '→', '↑']\n",
        "    action_arrows = {0: '←', 1: '↓', 2: '→', 3: '↑'}\n",
        "\n",
        "    # Create a figure and axis\n",
        "    fig, ax = plt.subplots(figsize=(8, 8))\n",
        "    ax.set_xlim(0, 8)\n",
        "    ax.set_ylim(0, 8)\n",
        "\n",
        "    for state in range(q_table.shape[0]):\n",
        "        # Get the best action for the current state\n",
        "        best_action = np.argmax(q_table[state])\n",
        "        arrow = action_arrows[best_action]\n",
        "\n",
        "        # Calculate the grid coordinates\n",
        "        row = state // 8\n",
        "        col = state % 8\n",
        "\n",
        "        # Display the arrow in the grid cell\n",
        "        ax.text(col + 0.5, 7 - row + 0.5, arrow, ha='center', va='center', fontsize=16)\n",
        "\n",
        "    # Draw grid lines\n",
        "    ax.set_xticks(np.arange(9))\n",
        "    ax.set_yticks(np.arange(9))\n",
        "    ax.grid(True)\n",
        "\n",
        "    # Remove ticks\n",
        "    ax.set_xticklabels([])\n",
        "    ax.set_yticklabels([])\n",
        "\n",
        "    plt.title('Final policy for FrozenLake-v1 (8x8)')\n",
        "    plt.savefig('fina_policy_frozen_lake8x8_avg.png')\n",
        "    plt.show()\n",
        "\n",
        "def run(episodes, num_runs=20, is_training=True, render=False, smooth_graph=False):\n",
        "    env = gym.make('FrozenLake-v1', map_name=\"8x8\", is_slippery=True, render_mode='human' if render else None)\n",
        "    all_rewards = []\n",
        "\n",
        "    # cargar la tabla Q promediada para la evaluación, si no está en entrenamiento\n",
        "    if not is_training:\n",
        "        if os.path.exists('frozen_lake8x8_avg.pkl'):\n",
        "            with open('frozen_lake8x8_avg.pkl', 'rb') as f:\n",
        "                q = pickle.load(f)\n",
        "        else:\n",
        "            print(\"No se encontró el archivo 'frozen_lake8x8_avg.pkl'. Ejecute en modo de entrenamiento primero.\")\n",
        "            return\n",
        "    else:\n",
        "        q = np.zeros((env.observation_space.n, env.action_space.n))  # inicializar la tabla Q\n",
        "\n",
        "    for run in range(num_runs):\n",
        "        if is_training:\n",
        "            q_run = np.zeros((env.observation_space.n, env.action_space.n))  # inicializar la tabla Q para cada ejecución\n",
        "\n",
        "        learning_rate_a = 0.01\n",
        "        discount_factor_g = 0.9\n",
        "        epsilon = 1\n",
        "        epsilon_decay_rate = 0.0000333\n",
        "        rng = np.random.default_rng()\n",
        "\n",
        "        rewards_per_episode = np.zeros(episodes)\n",
        "\n",
        "        for i in range(episodes):\n",
        "            state = env.reset()[0]\n",
        "            terminated = False\n",
        "            truncated = False\n",
        "            episode_reward = 0  # inicializar la recompensa del episodio\n",
        "\n",
        "            while not terminated and not truncated:\n",
        "                if is_training and rng.random() < epsilon:\n",
        "                    action = env.action_space.sample()\n",
        "                else:\n",
        "                    action = np.argmax(q_run[state, :] if is_training else q[state, :])\n",
        "\n",
        "                new_state, reward, terminated, truncated, _ = env.step(action)\n",
        "\n",
        "                if is_training:\n",
        "                    q_run[state, action] = q_run[state, action] + learning_rate_a * (\n",
        "                        reward + discount_factor_g * np.max(q_run[new_state, :]) - q_run[state, action]\n",
        "                    )\n",
        "\n",
        "                state = new_state\n",
        "                episode_reward += reward  # acumular la recompensa del episodio\n",
        "\n",
        "            if is_training:\n",
        "                epsilon = max(epsilon - epsilon_decay_rate, 0)\n",
        "                if epsilon == 0:\n",
        "                    learning_rate_a = 0.001\n",
        "\n",
        "            rewards_per_episode[i] += episode_reward  # sumar la recompensa acumulada del episodio\n",
        "\n",
        "        all_rewards.append(rewards_per_episode)\n",
        "\n",
        "        if is_training:\n",
        "            with open(f\"frozen_lake8x8_run{run}.pkl\", \"wb\") as f:\n",
        "                pickle.dump(q_run, f)\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    # promediar los resultados de todas las ejecuciones\n",
        "    if is_training:\n",
        "        all_rewards = np.array(all_rewards)\n",
        "        average_rewards = np.mean(all_rewards, axis=0)\n",
        "\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.plot(average_rewards, label='average rewards')\n",
        "        if smooth_graph:\n",
        "            average_rewards_smooth = smooth(average_rewards, 10)\n",
        "            plt.plot(range(len(average_rewards_smooth)), average_rewards_smooth, label='smoothed average rewards')\n",
        "        plt.xlabel('episodes')\n",
        "        plt.ylabel('average reward')\n",
        "        plt.title('average rewards per episode over training')\n",
        "        plt.xlim(0, len(average_rewards))\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.savefig('training_frozen_lake8x8_avg.png')\n",
        "        plt.show()\n",
        "\n",
        "        # guardar la tabla Q promediada\n",
        "        avg_q = np.mean([pickle.load(open(f\"frozen_lake8x8_run{run}.pkl\", 'rb')) for run in range(num_runs)], axis=0)\n",
        "        with open('frozen_lake8x8_avg.pkl', 'wb') as f:\n",
        "            pickle.dump(avg_q, f)\n",
        "\n",
        "    if not is_training:\n",
        "        plot_policy(q)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    is_training = False  # ajusta esto según sea necesario\n",
        "\n",
        "    if is_training:\n",
        "        smooth_input = input(\"¿Desea suavizar la gráfica? (y/n): \").strip().lower()\n",
        "        smooth_graph = smooth_input == 'y'\n",
        "    else:\n",
        "        smooth_graph = False\n",
        "\n",
        "    # para generar el promedio sobre 20 ejecuciones y 10,000 episodios, descomenta las siguientes líneas:\n",
        "    #run(30_000, num_runs=20, is_training=is_training, render=False, smooth_graph=smooth_graph)\n",
        "    run(1, num_runs=1,  is_training=is_training, render=True, smooth_graph=smooth_graph)  # evaluación usando la tabla Q promediada"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SARSA"
      ],
      "metadata": {
        "id": "5hyCzy0DJbZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "# SARSA\n",
        "\n",
        "def smooth(y, box_pts):\n",
        "    box = np.ones(box_pts) / box_pts\n",
        "    y_smooth = np.convolve(y, box, mode='valid')\n",
        "    return y_smooth\n",
        "\n",
        "def plot_policy(q_table):\n",
        "    actions = ['←', '↓', '→', '↑']\n",
        "    action_arrows = {0: '←', 1: '↓', 2: '→', 3: '↑'}\n",
        "\n",
        "    # Create a figure and axis\n",
        "    fig, ax = plt.subplots(figsize=(8, 8))\n",
        "    ax.set_xlim(0, 8)\n",
        "    ax.set_ylim(0, 8)\n",
        "\n",
        "    for state in range(q_table.shape[0]):\n",
        "        # Get the best action for the current state\n",
        "        best_action = np.argmax(q_table[state])\n",
        "        arrow = action_arrows[best_action]\n",
        "\n",
        "        # Calculate the grid coordinates\n",
        "        row = state // 8\n",
        "        col = state % 8\n",
        "\n",
        "        # Display the arrow in the grid cell\n",
        "        ax.text(col + 0.5, 7 - row + 0.5, arrow, ha='center', va='center', fontsize=16)\n",
        "\n",
        "    # Draw grid lines\n",
        "    ax.set_xticks(np.arange(9))\n",
        "    ax.set_yticks(np.arange(9))\n",
        "    ax.grid(True)\n",
        "\n",
        "    # Remove ticks\n",
        "    ax.set_xticklabels([])\n",
        "    ax.set_yticklabels([])\n",
        "\n",
        "    plt.title('Final policy for FrozenLake-v1 (8x8)')\n",
        "    plt.savefig('final_policy_frozen_lake8x8_avg.png')\n",
        "    plt.show()\n",
        "\n",
        "def run(episodes, num_runs=20, is_training=True, render=False, smooth_graph=False):\n",
        "    env = gym.make('FrozenLake-v1', map_name=\"8x8\", is_slippery=False, render_mode='human' if render else None)\n",
        "    all_rewards = []\n",
        "\n",
        "    # cargar la tabla Q promediada para la evaluación, si no está en entrenamiento\n",
        "    if not is_training:\n",
        "        if os.path.exists('frozen_lake8x8_avg.pkl'):\n",
        "            with open('frozen_lake8x8_avg.pkl', 'rb') as f:\n",
        "                q = pickle.load(f)\n",
        "        else:\n",
        "            print(\"No se encontró el archivo 'frozen_lake8x8_avg.pkl'. Ejecute en modo de entrenamiento primero.\")\n",
        "            return\n",
        "    else:\n",
        "        q = np.zeros((env.observation_space.n, env.action_space.n))  # inicializar la tabla Q\n",
        "\n",
        "    for run in range(num_runs):\n",
        "        if is_training:\n",
        "            q_run = np.zeros((env.observation_space.n, env.action_space.n))  # inicializar la tabla Q para cada ejecución\n",
        "\n",
        "        learning_rate_a = 0.01\n",
        "        discount_factor_g = 0.9\n",
        "        epsilon = 1\n",
        "        epsilon_decay_rate = 0.0000333\n",
        "        rng = np.random.default_rng()\n",
        "\n",
        "        rewards_per_episode = np.zeros(episodes)\n",
        "\n",
        "        for i in range(episodes):\n",
        "            state = env.reset()[0]\n",
        "            terminated = False\n",
        "            truncated = False\n",
        "            episode_reward = 0  # inicializar la recompensa del episodio\n",
        "\n",
        "            if is_training and rng.random() < epsilon:\n",
        "                action = env.action_space.sample()\n",
        "            else:\n",
        "                action = np.argmax(q_run[state, :] if is_training else q[state, :])\n",
        "\n",
        "            while not terminated and not truncated:\n",
        "                new_state, reward, terminated, truncated, _ = env.step(action)\n",
        "\n",
        "                if is_training and rng.random() < epsilon:\n",
        "                    next_action = env.action_space.sample()\n",
        "                else:\n",
        "                    next_action = np.argmax(q_run[new_state, :] if is_training else q[new_state, :])\n",
        "\n",
        "                if is_training:\n",
        "                    q_run[state, action] = q_run[state, action] + learning_rate_a * (\n",
        "                        reward + discount_factor_g * q_run[new_state, next_action] - q_run[state, action]\n",
        "                    )\n",
        "\n",
        "                state = new_state\n",
        "                action = next_action\n",
        "                episode_reward += reward  # acumular la recompensa del episodio\n",
        "\n",
        "            if is_training:\n",
        "                epsilon = max(epsilon - epsilon_decay_rate, 0)\n",
        "                if epsilon == 0:\n",
        "                    learning_rate_a = 0.001\n",
        "\n",
        "            rewards_per_episode[i] += episode_reward  # sumar la recompensa acumulada del episodio\n",
        "\n",
        "        all_rewards.append(rewards_per_episode)\n",
        "\n",
        "        if is_training:\n",
        "            with open(f\"frozen_lake8x8_run{run}.pkl\", \"wb\") as f:\n",
        "                pickle.dump(q_run, f)\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    # promediar los resultados de todas las ejecuciones\n",
        "    if is_training:\n",
        "        all_rewards = np.array(all_rewards)\n",
        "        average_rewards = np.mean(all_rewards, axis=0)\n",
        "\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.plot(average_rewards, label='average rewards')\n",
        "        if smooth_graph:\n",
        "            average_rewards_smooth = smooth(average_rewards, 10)\n",
        "            plt.plot(range(len(average_rewards_smooth)), average_rewards_smooth, label='smoothed average rewards')\n",
        "        plt.xlabel('episodes')\n",
        "        plt.ylabel('average reward')\n",
        "        plt.title('average rewards per episode over training')\n",
        "        plt.xlim(0, len(average_rewards))\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.savefig('training_frozen_lake8x8_avg.png')\n",
        "        plt.show()\n",
        "\n",
        "        # guardar la tabla Q promediada\n",
        "        avg_q = np.mean([pickle.load(open(f\"frozen_lake8x8_run{run}.pkl\", 'rb')) for run in range(num_runs)], axis=0)\n",
        "        with open('frozen_lake8x8_avg.pkl', 'wb') as f:\n",
        "            pickle.dump(avg_q, f)\n",
        "\n",
        "    if not is_training:\n",
        "        plot_policy(q)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    is_training = True  # ajusta esto según sea necesario\n",
        "\n",
        "    if is_training:\n",
        "        smooth_input = input(\"¿Desea suavizar la gráfica? (y/n): \").strip().lower()\n",
        "        smooth_graph = smooth_input == 'y'\n",
        "    else:\n",
        "        smooth_graph = False\n",
        "\n",
        "    # para generar el promedio sobre 20 ejecuciones y 10,000 episodios, descomenta las siguientes líneas:\n",
        "    run(30_000, num_runs=20, is_training=is_training, render=False, smooth_graph=smooth_graph)\n",
        "    #run(1, num_runs=1,  is_training=is_training, render=True, smooth_graph=smooth_graph)  # evaluación usando la tabla Q promediada"
      ],
      "metadata": {
        "id": "BCstRuCCKwXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Expected SARSA"
      ],
      "metadata": {
        "id": "cmvYwOx9Jd8P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "# Expected SARSA\n",
        "\n",
        "def smooth(y, box_pts):\n",
        "    box = np.ones(box_pts) / box_pts\n",
        "    y_smooth = np.convolve(y, box, mode='valid')\n",
        "    return y_smooth\n",
        "\n",
        "def plot_policy(q_table):\n",
        "    actions = ['←', '↓', '→', '↑']\n",
        "    action_arrows = {0: '←', 1: '↓', 2: '→', 3: '↑'}\n",
        "\n",
        "    # Create a figure and axis\n",
        "    fig, ax = plt.subplots(figsize=(8, 8))\n",
        "    ax.set_xlim(0, 8)\n",
        "    ax.set_ylim(0, 8)\n",
        "\n",
        "    for state in range(q_table.shape[0]):\n",
        "        # Get the best action for the current state\n",
        "        best_action = np.argmax(q_table[state])\n",
        "        arrow = action_arrows[best_action]\n",
        "\n",
        "        # Calculate the grid coordinates\n",
        "        row = state // 8\n",
        "        col = state % 8\n",
        "\n",
        "        # Display the arrow in the grid cell\n",
        "        ax.text(col + 0.5, 7 - row + 0.5, arrow, ha='center', va='center', fontsize=16)\n",
        "\n",
        "    # Draw grid lines\n",
        "    ax.set_xticks(np.arange(9))\n",
        "    ax.set_yticks(np.arange(9))\n",
        "    ax.grid(True)\n",
        "\n",
        "    # Remove ticks\n",
        "    ax.set_xticklabels([])\n",
        "    ax.set_yticklabels([])\n",
        "\n",
        "    plt.title('Final policy for FrozenLake-v1 (8x8)')\n",
        "    plt.savefig('final_policy_frozen_lake8x8_avg.png')\n",
        "    plt.show()\n",
        "\n",
        "def run(episodes, num_runs=20, is_training=True, render=False, smooth_graph=False):\n",
        "    env = gym.make('FrozenLake-v1', map_name=\"8x8\", is_slippery=False, render_mode='human' if render else None)\n",
        "    all_rewards = []\n",
        "\n",
        "    # cargar la tabla Q promediada para la evaluación, si no está en entrenamiento\n",
        "    if not is_training:\n",
        "        if os.path.exists('frozen_lake8x8_avg.pkl'):\n",
        "            with open('frozen_lake8x8_avg.pkl', 'rb') as f:\n",
        "                q = pickle.load(f)\n",
        "        else:\n",
        "            print(\"No se encontró el archivo 'frozen_lake8x8_avg.pkl'. Ejecute en modo de entrenamiento primero.\")\n",
        "            return\n",
        "    else:\n",
        "        q = np.zeros((env.observation_space.n, env.action_space.n))  # inicializar la tabla Q\n",
        "\n",
        "    for run in range(num_runs):\n",
        "        if is_training:\n",
        "            q_run = np.zeros((env.observation_space.n, env.action_space.n))  # inicializar la tabla Q para cada ejecución\n",
        "\n",
        "        learning_rate_a = 0.01\n",
        "        discount_factor_g = 0.9\n",
        "        epsilon = 1\n",
        "        epsilon_decay_rate = 0.0000333\n",
        "        rng = np.random.default_rng()\n",
        "\n",
        "        rewards_per_episode = np.zeros(episodes)\n",
        "\n",
        "        for i in range(episodes):\n",
        "            state = env.reset()[0]\n",
        "            terminated = False\n",
        "            truncated = False\n",
        "            episode_reward = 0  # inicializar la recompensa del episodio\n",
        "\n",
        "            while not terminated and not truncated:\n",
        "                if is_training and rng.random() < epsilon:\n",
        "                    action = env.action_space.sample()\n",
        "                else:\n",
        "                    action = np.argmax(q_run[state, :] if is_training else q[state, :])\n",
        "\n",
        "                new_state, reward, terminated, truncated, _ = env.step(action)\n",
        "\n",
        "                if is_training:\n",
        "                    # Calcular el valor esperado de Q en el siguiente estado\n",
        "                    policy = np.ones(env.action_space.n) * epsilon / env.action_space.n\n",
        "                    best_action = np.argmax(q_run[new_state, :])\n",
        "                    policy[best_action] += (1.0 - epsilon)\n",
        "                    expected_q = np.sum(policy * q_run[new_state, :])\n",
        "\n",
        "                    # Actualizar la tabla Q usando el valor esperado\n",
        "                    q_run[state, action] = q_run[state, action] + learning_rate_a * (\n",
        "                        reward + discount_factor_g * expected_q - q_run[state, action]\n",
        "                    )\n",
        "\n",
        "                state = new_state\n",
        "                episode_reward += reward  # acumular la recompensa del episodio\n",
        "\n",
        "            if is_training:\n",
        "                epsilon = max(epsilon - epsilon_decay_rate, 0)\n",
        "                if epsilon == 0:\n",
        "                    learning_rate_a = 0.001\n",
        "\n",
        "            rewards_per_episode[i] += episode_reward  # sumar la recompensa acumulada del episodio\n",
        "\n",
        "        all_rewards.append(rewards_per_episode)\n",
        "\n",
        "        if is_training:\n",
        "            with open(f\"frozen_lake8x8_run{run}.pkl\", \"wb\") as f:\n",
        "                pickle.dump(q_run, f)\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    # promediar los resultados de todas las ejecuciones\n",
        "    if is_training:\n",
        "        all_rewards = np.array(all_rewards)\n",
        "        average_rewards = np.mean(all_rewards, axis=0)\n",
        "\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.plot(average_rewards, label='average rewards')\n",
        "        if smooth_graph:\n",
        "            average_rewards_smooth = smooth(average_rewards, 10)\n",
        "            plt.plot(range(len(average_rewards_smooth)), average_rewards_smooth, label='smoothed average rewards')\n",
        "        plt.xlabel('episodes')\n",
        "        plt.ylabel('average reward')\n",
        "        plt.title('average rewards per episode over training')\n",
        "        plt.xlim(0, len(average_rewards))\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.savefig('training_frozen_lake8x8_avg.png')\n",
        "        plt.show()\n",
        "\n",
        "        # guardar la tabla Q promediada\n",
        "        avg_q = np.mean([pickle.load(open(f\"frozen_lake8x8_run{run}.pkl\", 'rb')) for run in range(num_runs)], axis=0)\n",
        "        with open('frozen_lake8x8_avg.pkl', 'wb') as f:\n",
        "            pickle.dump(avg_q, f)\n",
        "\n",
        "    if not is_training:\n",
        "        plot_policy(q)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    is_training = True  # ajusta esto según sea necesario\n",
        "\n",
        "    if is_training:\n",
        "        smooth_input = input(\"¿Desea suavizar la gráfica? (y/n): \").strip().lower()\n",
        "        smooth_graph = smooth_input == 'y'\n",
        "    else:\n",
        "        smooth_graph = False\n",
        "\n",
        "    # para generar el promedio sobre 20 ejecuciones y 10,000 episodios, descomenta las siguientes líneas:\n",
        "    run(30_000, num_runs=20, is_training=is_training, render=False, smooth_graph=smooth_graph)\n",
        "    #run(1, num_runs=1,  is_training=is_training, render=True, smooth_graph=smooth_graph)  # evaluación usando la tabla Q promediada"
      ],
      "metadata": {
        "id": "RHkGlwY_KxMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Monte Carlo First Visit with Exploring Starts"
      ],
      "metadata": {
        "id": "5QVWAc3bYUTW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "def smooth(y, box_pts):\n",
        "    box = np.ones(box_pts) / box_pts\n",
        "    y_smooth = np.convolve(y, box, mode='valid')\n",
        "    return y_smooth\n",
        "\n",
        "def plot_policy(q_table):\n",
        "    actions = ['←', '↓', '→', '↑']\n",
        "    action_arrows = {0: '←', 1: '↓', 2: '→', 3: '↑'}\n",
        "\n",
        "    # Create a figure and axis\n",
        "    fig, ax = plt.subplots(figsize=(8, 8))\n",
        "    ax.set_xlim(0, 8)\n",
        "    ax.set_ylim(0, 8)\n",
        "\n",
        "    for state in range(q_table.shape[0]):\n",
        "        # Get the best action for the current state\n",
        "        best_action = np.argmax(q_table[state])\n",
        "        arrow = action_arrows[best_action]\n",
        "\n",
        "        # Calculate the grid coordinates\n",
        "        row = state // 8\n",
        "        col = state % 8\n",
        "\n",
        "        # Display the arrow in the grid cell\n",
        "        ax.text(col + 0.5, 7 - row + 0.5, arrow, ha='center', va='center', fontsize=16)\n",
        "\n",
        "    # Draw grid lines\n",
        "    ax.set_xticks(np.arange(9))\n",
        "    ax.set_yticks(np.arange(9))\n",
        "    ax.grid(True)\n",
        "\n",
        "    # Remove ticks\n",
        "    ax.set_xticklabels([])\n",
        "    ax.set_yticklabels([])\n",
        "\n",
        "    plt.title('Final policy for FrozenLake-v1 (8x8)')\n",
        "    plt.savefig('final_policy_frozen_lake8x8_avg.png')\n",
        "    plt.show()\n",
        "\n",
        "def run(episodes, num_runs=20, is_training=True, render=False, smooth_graph=False):\n",
        "    env = gym.make('FrozenLake-v1', map_name=\"8x8\", is_slippery=False, render_mode='human' if render else None)\n",
        "    all_rewards = []\n",
        "\n",
        "    # cargar la tabla Q promediada para la evaluación, si no está en entrenamiento\n",
        "    if not is_training:\n",
        "        if os.path.exists('first_visit_mc_es_avg.pkl'):\n",
        "            with open('first_visit_mc_es_avg.pkl', 'rb') as f:\n",
        "                q = pickle.load(f)\n",
        "        else:\n",
        "            print(\"No se encontró el archivo 'first_visit_mc_es_avg.pkl'. Ejecute en modo de entrenamiento primero.\")\n",
        "            return\n",
        "    else:\n",
        "        q = np.zeros((env.observation_space.n, env.action_space.n))  # inicializar la tabla Q\n",
        "\n",
        "    for run in range(num_runs):\n",
        "        if is_training:\n",
        "            q_run = np.zeros((env.observation_space.n, env.action_space.n))  # inicializar la tabla Q para cada ejecución\n",
        "            returns = {state: {action: [] for action in range(env.action_space.n)} for state in range(env.observation_space.n)}\n",
        "        else:\n",
        "            q_run = q  # en modo evaluación, usamos la tabla Q promediada\n",
        "\n",
        "        epsilon = 1 if is_training else 0  # solo se usa epsilon en modo de entrenamiento\n",
        "        epsilon_decay_rate = 0.0000333\n",
        "        rng = np.random.default_rng()\n",
        "\n",
        "        rewards_per_episode = np.zeros(episodes)\n",
        "\n",
        "        for i in range(episodes):\n",
        "            if is_training:\n",
        "                # Exploring starts: inicializar el episodio desde un estado-acción aleatorio\n",
        "                state = env.observation_space.sample()\n",
        "                action = env.action_space.sample()\n",
        "                new_state, reward, terminated, truncated, _ = env.step(action)\n",
        "                episode = [(state, action, reward)]\n",
        "                state = new_state\n",
        "                episode_reward = reward\n",
        "            else:\n",
        "                state = env.reset()[0]\n",
        "                episode = []\n",
        "                episode_reward = 0\n",
        "\n",
        "            terminated = False\n",
        "            truncated = False\n",
        "\n",
        "            while not terminated and not truncated:\n",
        "                if is_training and rng.random() < epsilon:\n",
        "                    action = env.action_space.sample()\n",
        "                else:\n",
        "                    action = np.argmax(q_run[state, :])\n",
        "\n",
        "                new_state, reward, terminated, truncated, _ = env.step(action)\n",
        "                episode.append((state, action, reward))\n",
        "                state = new_state\n",
        "                episode_reward += reward  # acumular la recompensa del episodio\n",
        "            # First visit\n",
        "            visited = set()\n",
        "            G = 0\n",
        "            for state, action, reward in reversed(episode):\n",
        "                G = reward + 0.9 * G  # discount_factor_g\n",
        "                if (state, action) not in visited:\n",
        "                    if is_training:\n",
        "                        returns[state][action].append(G)\n",
        "                        q_run[state, action] = np.mean(returns[state][action])\n",
        "                    visited.add((state, action))\n",
        "\n",
        "            if is_training:\n",
        "                epsilon = max(epsilon - epsilon_decay_rate, 0)\n",
        "\n",
        "            rewards_per_episode[i] += episode_reward  # sumar la recompensa acumulada del episodio\n",
        "\n",
        "        all_rewards.append(rewards_per_episode)\n",
        "\n",
        "        if is_training:\n",
        "            with open(f\"first_visit_mc_es_run{run}.pkl\", \"wb\") as f:\n",
        "                pickle.dump(q_run, f)\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    # promediar los resultados de todas las ejecuciones\n",
        "    if is_training:\n",
        "        all_rewards = np.array(all_rewards)\n",
        "        average_rewards = np.mean(all_rewards, axis=0)\n",
        "\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.plot(average_rewards, label='average rewards')\n",
        "        if smooth_graph:\n",
        "            average_rewards_smooth = smooth(average_rewards, 10)\n",
        "            plt.plot(range(len(average_rewards_smooth)), average_rewards_smooth, label='smoothed average rewards')\n",
        "        plt.xlabel('episodes')\n",
        "        plt.ylabel('average reward')\n",
        "        plt.title('average rewards per episode over training')\n",
        "        plt.xlim(0, len(average_rewards))\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.savefig('training_first_visit_mc_es_avg.png')\n",
        "        plt.show()\n",
        "\n",
        "        # guardar la tabla Q promediada\n",
        "        avg_q = np.mean([pickle.load(open(f\"first_visit_mc_es_run{run}.pkl\", 'rb')) for run in range(num_runs)], axis=0)\n",
        "        with open('first_visit_mc_es_avg.pkl', 'wb') as f:\n",
        "            pickle.dump(avg_q, f)\n",
        "\n",
        "    if not is_training:\n",
        "        plot_policy(q)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    is_training = False  # ajusta esto según sea necesario\n",
        "\n",
        "    if is_training:\n",
        "        smooth_input = input(\"¿Desea suavizar la gráfica? (y/n): \").strip().lower()\n",
        "        smooth_graph = smooth_input == 'y'\n",
        "    else:\n",
        "        smooth_graph = False\n",
        "\n",
        "    # para generar el promedio sobre 20 ejecuciones y 30,000 episodios, descomenta las siguientes líneas:\n",
        "    #run(30_000, num_runs=20, is_training=is_training, render=False, smooth_graph=smooth_graph)\n",
        "    run(1, num_runs=1, is_training=is_training, render=True, smooth_graph=smooth_graph)  # evaluación usando la tabla Q promediada"
      ],
      "metadata": {
        "id": "uLkXVjnoYZGY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}